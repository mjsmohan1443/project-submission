# -*- coding: utf-8 -*-
"""Phase-3-code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ibH7Pmy1605p1ofwQ1LBex_LKPviA_xe

upload the dataset
"""

from google.colab import files
uploaded = files.upload()

"""load the dataset"""

from IPython import get_ipython
from IPython.display import display
# %%
from google.colab import files
uploaded = files.upload()
# %%
import pandas as pd

# Get the filename from the uploaded dictionary
# files.upload() returns a dictionary where keys are filenames and values are file contents
# Assuming only one file was uploaded, get the first key (the filename)
if uploaded:
  file_name = list(uploaded.keys())[0]

  # Load the dataset using the uploaded filename
  news_df = pd.read_csv(file_name)

  # Print basic info and a sample of the data
  print("Dataset Information:")
  print(news_df.info())
  print("\nSample Data:")
  print(news_df.head())
else:
  print("No file was uploaded.")

"""Data Exploration"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# The dataset 'news_df' is already loaded in the previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv'
# news_df = pd.read_csv(file_path)

# 1. Checking for missing values and duplicates
print("üîç Checking for Missing Values and Duplicates")
print("Missing Values:\n", news_df.isna().sum())
print("\nDuplicate Rows:", news_df.duplicated().sum())

# 2. Label distribution
print("\nüîç Label Distribution")
label_counts = news_df['label'].value_counts()
print(label_counts)

# Plotting label distribution
plt.figure(figsize=(6,4))
sns.barplot(x=label_counts.index, y=label_counts.values, palette='viridis')
plt.title('Label Distribution (Fake vs Real)')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# 3. Text length distribution
print("\nüîç Text Length Distribution")
news_df['text_length'] = news_df['Text'].apply(len)
plt.figure(figsize=(8,5))
sns.histplot(news_df['text_length'], bins=30, kde=True, color='teal')
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

# 4. Sample texts for inspection
print("\nüîç Sample Fake News Articles")
print(news_df[news_df['label'] == 'Fake'].sample(2)['Text'].values)

print("\nüîç Sample Real News Articles")
print(news_df[news_df['label'] == 'Real'].sample(2)['Text'].values)

"""Check for Missing Values and Duplicates"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# The dataset 'news_df' is already loaded in the previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv' # <-- Remove this line
# news_df = pd.read_csv(file_path) # <-- Remove this line

# Checking for missing values
print("üîç Checking for Missing Values")
missing_values = news_df.isna().sum()
print(missing_values)

# Checking for duplicates
print("\nüîç Checking for Duplicates")
duplicate_count = news_df.duplicated().sum()
print("Duplicate Rows:", duplicate_count)

# Optionally, remove duplicates
if duplicate_count > 0:
    print("\nüßπ Removing Duplicates...")
    news_df.drop_duplicates(inplace=True)
    print("Duplicates removed. New dataset shape:", news_df.shape)
else:
    print("\n‚úÖ No Duplicates Found")

"""Visualize a Few Features"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter

# Load the dataset
# The dataset 'news_df' is already loaded in the previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv' # <-- Remove this line
# news_df = pd.read_csv(file_path) # <-- Remove this line


# 1. Label Distribution
plt.figure(figsize=(6,4))
# Ensure news_df is accessible from the previous cell
# If running this cell independently, you would need to re-run the file upload cell first.
sns.countplot(data=news_df, x='label', palette='viridis')
plt.title('Label Distribution (Fake vs Real)')
plt.xlabel('Label')
plt.ylabel('Count')
plt.show()

# 2. Text Length Distribution
# Re-calculate text_length if the notebook state was reset or run out of order
if 'text_length' not in news_df.columns:
    news_df['text_length'] = news_df['Text'].apply(len)

plt.figure(figsize=(8,5))
sns.histplot(news_df['text_length'], bins=30, kde=True, color='teal')
plt.title('Distribution of Text Lengths')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

# 3. Word Cloud for Fake News
# Ensure the 'Text' column exists
if 'Text' in news_df.columns and 'label' in news_df.columns:
    fake_text = " ".join(news_df[news_df['label'] == 'Fake']['Text'].dropna().values)
    if fake_text: # Only generate wordcloud if there is text
        fake_wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(fake_text)
        plt.figure(figsize=(10,5))
        plt.title("Most Common Words in Fake News")
        plt.imshow(fake_wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.show()
    else:
        print("No text available for Fake news word cloud.")
else:
    print("Required columns ('Text' and 'label') not found for word cloud generation.")


# 4. Word Cloud for Real News
# Ensure the 'Text' column exists
if 'Text' in news_df.columns and 'label' in news_df.columns:
    real_text = " ".join(news_df[news_df['label'] == 'Real']['Text'].dropna().values)
    if real_text: # Only generate wordcloud if there is text
        real_wordcloud = WordCloud(width=800, height=400, background_color='black', colormap='Blues').generate(real_text)
        plt.figure(figsize=(10,5))
        plt.title("Most Common Words in Real News")
        plt.imshow(real_wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.show()
    else:
        print("No text available for Real news word cloud.")
else:
    print("Required columns ('Text' and 'label') not found for word cloud generation.")

"""Identify Target and Features"""

import pandas as pd

# Load the dataset
# The dataset 'news_df' is already loaded in a previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv' # <-- Remove this line
# news_df = pd.read_csv(file_path) # <-- Remove this line

# Identifying the Features and Target
X = news_df['Text']  # Features (news articles)
y = news_df['label']  # Target (Fake or Real)

# Display the shape and first few rows to verify
print("Features (X) Shape:", X.shape)
print("Target (y) Shape:", y.shape)
print("\nSample Features (X):")
print(X.head())
print("\nSample Target (y):")
print(y.head())

"""Convert Categorical Columns to Numerical"""

import pandas as pd

# Load the dataset
# The dataset 'news_df' is already loaded in a previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv' # <-- Remove this line
# news_df = pd.read_csv(file_path) # <-- Remove this line

# Convert the 'label' column to numerical values
# Fake -> 0, Real -> 1
news_df['label_num'] = news_df['label'].map({'Fake': 0, 'Real': 1})

# Verify the conversion
print("‚úÖ Conversion Complete")
print("\nSample Data:")
print(news_df[['label', 'label_num']].head())

# Check the distribution
print("\nüîç Label Distribution:")
print(news_df['label_num'].value_counts())

"""One -Hot Encoding"""

import pandas as pd

# Load the dataset
# The dataset 'news_df' is already loaded in a previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv' # <-- Remove this line
# news_df = pd.read_csv(file_path) # <-- Remove this line

# One-Hot Encode the 'label' column
encoded_labels = pd.get_dummies(news_df['label'], prefix='label')

# Combine the encoded labels with the original dataframe
# Use .copy() to avoid SettingWithCopyWarning later if modifying news_df
news_df = news_df.copy()
news_df = pd.concat([news_df, encoded_labels], axis=1)


# Verify the encoding
print("‚úÖ One-Hot Encoding Complete")
print("\nSample Data:")
print(news_df[['label', 'label_Fake', 'label_Real']].head())

# Check distribution
print("\nüîç Encoded Label Distribution:")
print(news_df[['label_Fake', 'label_Real']].sum())

"""Feature Scaling"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from scipy.sparse import csr_matrix

# The dataset 'news_df' is already loaded in a previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv' # <-- Remove this line
# news_df = pd.read_csv(file_path) # <-- Remove this line

# Step 1: Vectorize text using TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
# Use the news_df loaded from the upload mechanism
X_tfidf = tfidf_vectorizer.fit_transform(news_df['Text'])

# Step 2: Feature scaling
# StandardScaler works with dense arrays, so convert sparse matrix to dense first (may be large)
# Check if X_tfidf is a sparse matrix before converting
if isinstance(X_tfidf, csr_matrix):
    X_dense = X_tfidf.toarray()
else:
    X_dense = X_tfidf # If it's already dense, use it directly


scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_dense)

print("Original TF-IDF shape:", X_tfidf.shape)
print("Scaled features shape:", X_scaled.shape)
print("\nSample scaled feature values (first row):")
print(X_scaled[0][:10])  # print first 10 features of first sample

"""Train-Test Split"""

import pandas as pd
from sklearn.model_selection import train_test_split

# The dataset 'news_df' is already loaded in a previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv' # <-- Remove this line
# news_df = pd.read_csv(file_path) # <-- Remove this line

# Features and target are already defined from the previously loaded news_df
# X = news_df['Text']
# y = news_df['label']

# Split the dataset: 80% train, 20% test, with stratification to keep label balance
# Use the X and y defined from the uploaded file
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Display the shape of the splits
print(f"Training set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")

# Optionally check label distribution in splits
print("\nTraining set label distribution:")
print(y_train.value_counts(normalize=True))
print("\nTesting set label distribution:")
print(y_test.value_counts(normalize=True))

"""Model Building"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Load dataset - The dataset 'news_df' is already loaded in a previous cell using files.upload()
# Remove the lines that attempt to load from a hardcoded path:
# file_path = '/mnt/data/fake_and_real_news (1).csv' # <-- Remove this line
# news_df = pd.read_csv(file_path) # <-- Remove this line

# Features and target
# Use the news_df DataFrame that is already loaded from the upload
X = news_df['Text']
y = news_df['label'].map({'Fake': 0, 'Real': 1})  # Convert to numerical labels

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Vectorize text data
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Train Logistic Regression model
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_tfidf, y_train)

# Predict on test data
y_pred = model.predict(X_test_tfidf)

# Evaluate model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
# Ensure target_names match the numerical mapping 0: Fake, 1: Real
print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))

"""Evaluation"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assume y_test and y_pred are defined from your model prediction step

# 1. Accuracy
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.4f}\n")

# 2. Classification Report (Precision, Recall, F1-Score)
print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))

# 3. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""Make Predictions From New Input"""

# Example: Assuming you already have a trained model and vectorizer

def predict_news(texts, model, vectorizer):
    """
    Predicts whether the input news texts are Fake or Real.

    Args:
        texts (list or str): A single news string or list of news strings.
        model: Trained classification model.
        vectorizer: Text vectorizer used in training (e.g., TF-IDF).

    Returns:
        List of predictions: 'Fake' or 'Real' for each input text.
    """
    if isinstance(texts, str):
        texts = [texts]  # Convert single string to list

    # Vectorize the input texts
    text_vectors = vectorizer.transform(texts)

    # Predict labels (numerical)
    preds_num = model.predict(text_vectors)

    # Map numerical labels back to categorical
    label_map = {0: 'Fake', 1: 'Real'}
    preds = [label_map[p] for p in preds_num]

    return preds

# --- Usage example ---

new_texts = [
    "Breaking: New study shows the benefits of meditation.",
    "Government secretly controls weather with special machines."
]

# Assuming 'model' and 'vectorizer' are already defined and trained
predictions = predict_news(new_texts, model, vectorizer)

for text, pred in zip(new_texts, predictions):
    print(f"News: {text}\nPrediction: {pred}\n")

"""Convert to data Frame and Encode"""

import pandas as pd

# Example data: list of news texts and their labels (optional)
news_texts = [
    "Top Trump Surrogate BRUTALLY Stabs Him In The Back",
    "U.S. conservative leader optimistic of common ground with Trump",
    "Court Forces Ohio To Allow Millions Of Illegal Voters",
    "Trump proposes U.S. tax overhaul, stirs concern among lobbyists"
]

labels = ['Fake', 'Real', 'Fake', 'Real']  # Optional, if you have labels

# Step 1: Convert to DataFrame
data = {'Text': news_texts}
if labels:
    data['label'] = labels

df = pd.DataFrame(data)

# Step 2: Encode 'label' column to numerical values if it exists
if 'label' in df.columns:
    df['label_num'] = df['label'].map({'Fake': 0, 'Real': 1})

# Display the DataFrame
print(df)

"""Predict the Final Grade"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Sample dataset: Features and Final Grade
data = {
    'Homework': [85, 90, 78, 92, 88, 76, 95, 89, 84, 91],
    'Midterm': [80, 85, 75, 90, 86, 70, 93, 87, 83, 88],
    'Attendance': [90, 95, 85, 98, 92, 80, 97, 91, 88, 94],
    'Final_Grade': [88, 92, 80, 94, 90, 75, 96, 90, 85, 93]
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Features and target
X = df[['Homework', 'Midterm', 'Attendance']]
y = df['Final_Grade']

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R^2 Score:", r2_score(y_test, y_pred))

# Predict final grade for a new student
new_student = [[88, 90, 93]]  # Homework, Midterm, Attendance scores
predicted_grade = model.predict(new_student)[0]
print(f"Predicted Final Grade for new student: {predicted_grade:.2f}")

!pip install streamlit

import pickle

# After training your model and vectorizer (example)
with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

with open('fake_news_model.pkl', 'wb') as f:
    pickle.dump(model, f)

pip install streamlit

"""Deployment-Building An Interactive App"""

!pip install gradio

import gradio as gr
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Sample training data
data = {
    'Text': [
        "Top Trump Surrogate BRUTALLY Stabs Him In The Back",
        "U.S. conservative leader optimistic of common ground with Trump",
        "Court Forces Ohio To Allow Millions Of Illegal Voters",
        "Trump proposes U.S. tax overhaul, stirs concern among lobbyists"
    ],
    'label': ['Fake', 'Real', 'Fake', 'Real']
}

# Prepare dataset
df = pd.DataFrame(data)
X = df['Text']
y = df['label'].map({'Fake': 0, 'Real': 1})

# Train TF-IDF vectorizer and Logistic Regression model
# Note: This part retrains the model and vectorizer using only the sample data.
# If you intended to use the model trained on the uploaded data, you would need to
# load the saved model and vectorizer files ('fake_news_model.pkl', 'tfidf_vectorizer.pkl')
# instead of retraining here. Assuming for now this is a self-contained example for the Gradio part.
vectorizer = TfidfVectorizer(stop_words='english')
X_vec = vectorizer.fit_transform(X)

model = LogisticRegression()
model.fit(X_vec, y)

# Prediction function
def predict_news(text):
    vec = vectorizer.transform([text])
    pred_num = model.predict(vec)[0]
    return "Real" if pred_num == 1 else "Fake"

# Build Gradio Interface
iface = gr.Interface(
    fn=predict_news,
    inputs=gr.Textbox(lines=6, placeholder="Enter news article text here..."),
    outputs=gr.Label(num_top_classes=2),
    title="Fake News Detection",
    description="Enter a news article below, and the model will predict whether it is Real or Fake news.",
    examples=[
        ["Top Trump Surrogate BRUTALLY Stabs Him In The Back"],
        ["U.S. conservative leader optimistic of common ground with Trump"],
        ["Court Forces Ohio To Allow Millions Of Illegal Voters"],
        ["Trump proposes U.S. tax overhaul, stirs concern among lobbyists"]
    ]
)

iface.launch()